{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cf785ed-0bdb-45e7-800b-16902447ee01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhan3275/env/torch2/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import math, os, sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import mamba_ssm\n",
    "from mamba_ssm import Mamba\n",
    "from mamba_ssm.ops.selective_scan_interface import selective_scan_fn, selective_scan_ref\n",
    "# from mamba_ssm.modules.mamba_simple import Mamba\n",
    "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
    "from einops import repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20d0016f-6885-4188-b5c6-ba6702958793",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "a = torch.randn((4,128,30,32,32), device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "11ce9efa-72c7-4d2b-82b7-fba429643d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "B, C, T, H, W = a.shape\n",
    "L = T * H * W\n",
    "d_state = 16\n",
    "dt_rank = 8\n",
    "d = 128\n",
    "K = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fc049a22-a306-4bc8-b52c-cc9476250804",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = a.view(B, K, -1, L).contiguous()\n",
    "dts = torch.randn((B, K, d, L), device=device).view(B, -1, L)\n",
    "Bs = torch.randn((B, K, d_state, L), device=device)\n",
    "Cs = torch.randn((B, K, d_state, L), device=device)\n",
    "D = torch.ones((K, d), device=device)\n",
    "Ds = torch.randn((K, d), device=device).view(-1)\n",
    "dt_projs_bias = torch.randn((K, d), device=device).view(-1)\n",
    "As = torch.randn((K*d, d_state), device=device)\n",
    "xs = xs.float().view(B, -1, L)\n",
    "Ds = nn.Parameter(Ds)\n",
    "As = nn.Parameter(As)\n",
    "dt_projs_bias = nn.Parameter(dt_projs_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1888170b-9424-4cba-85b5-c0966cd6719f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128, 30720])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c3b0827f-6f59-4c30-b91d-9a98e1236d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_y = selective_scan_fn(\n",
    "    xs, dts,\n",
    "    As, Bs, Cs, Ds, z=None,\n",
    "    delta_bias=dt_projs_bias,\n",
    "    delta_softplus=True,\n",
    "    return_last_state=False,\n",
    ")\n",
    "#.view(B, K, -1, L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e238e969-2b98-47c4-8933-5871bc438321",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_event = torch.cuda.Event(enable_timing=True)\n",
    "end_event = torch.cuda.Event(enable_timing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "274d0828-457e-4d05-8a4a-8dad91c744c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 12936.4990234375 ms\n"
     ]
    }
   ],
   "source": [
    "# Record start event\n",
    "start_event.record()\n",
    "\n",
    "# Run the operation\n",
    "for i in range(100):\n",
    "    out_y = selective_scan_fn(\n",
    "        xs, dts,\n",
    "        As, Bs, Cs, Ds, z=None,\n",
    "        delta_bias=dt_projs_bias,\n",
    "        delta_softplus=True,\n",
    "        return_last_state=False,\n",
    "    )\n",
    "    y_sum = torch.sum(out_y)\n",
    "    y_sum.backward()\n",
    "# Record end event\n",
    "end_event.record()\n",
    "\n",
    "# Wait for everything to finish running\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "# Calculate elapsed time in milliseconds\n",
    "elapsed_time_ms = start_event.elapsed_time(end_event)\n",
    "print(f\"Elapsed time: {elapsed_time_ms} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5a865ec6-f1ef-46df-8c6f-86b64ca4b364",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DWConv(nn.Module):\n",
    "    def __init__(self, dim=768):\n",
    "        super(DWConv, self).__init__()\n",
    "        self.dwconv = nn.Conv3d(dim, dim, 3, 1, 1, bias=True, groups=dim)\n",
    "\n",
    "    def forward(self, x, nf, H, W):\n",
    "        B, N, C = x.shape\n",
    "        x = x.transpose(1, 2).contiguous().reshape(B, C, nf, H, W)\n",
    "        x = self.dwconv(x)\n",
    "        x = x.contiguous().flatten(2).transpose(1, 2)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class deconv(nn.Module):\n",
    "    def __init__(self, input_channel, output_channel, kernel_size=3, padding=0):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(input_channel, output_channel,\n",
    "                              kernel_size=kernel_size, stride=1, padding=padding)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.interpolate(x, scale_factor=2, mode='bilinear',\n",
    "                          align_corners=True)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.dwconv = DWConv(hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "        elif isinstance(m, nn.Conv2d):\n",
    "            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "            fan_out //= m.groups\n",
    "            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x, nf, H, W):\n",
    "        x = self.fc1(x)\n",
    "        x = self.dwconv(x, nf, H, W)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MambaLayerglobal(nn.Module):\n",
    "    def __init__(self, dim, d_state=16, d_conv=4, expand=2, mlp_ratio=4, drop=0., drop_path=0., act_layer=nn.GELU,\n",
    "                 reverse=True):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.mamba = Mamba(\n",
    "            d_model=dim,  # Model dimension d_model\n",
    "            d_state=d_state,  # SSM state expansion factor\n",
    "            d_conv=d_conv,  # Local convolution width\n",
    "            expand=expand,  # Block expansion factor\n",
    "            # use_fast_path=False,\n",
    "        )\n",
    "\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "        self.reverse = reverse\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "        elif isinstance(m, nn.Conv2d):\n",
    "            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "            fan_out //= m.groups\n",
    "            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # x = x.permute(0, 2, 1, 3, 4)\n",
    "        B, C, nf, H, W = x.shape\n",
    "\n",
    "        assert C == self.dim\n",
    "\n",
    "        n_tokens = x.shape[2:].numel()\n",
    "\n",
    "        img_dims = x.shape[2:]\n",
    "\n",
    "        x_flat = x.reshape(B, C, n_tokens).transpose(-1, -2)\n",
    "\n",
    "        # Bi-Mamba layer\n",
    "        x_mamba = x_flat + self.drop_path(self.mamba(self.norm1(x_flat)))\n",
    "        x_mamba = x_mamba + self.drop_path(self.mlp(self.norm2(x_mamba), nf, H, W))\n",
    "\n",
    "        out = x_mamba.transpose(-1, -2).reshape(B, C, *img_dims)\n",
    "\n",
    "        out = out.permute(0, 2, 1, 3, 4)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a8d56e22-63b1-43cb-9dae-312513da3688",
   "metadata": {},
   "outputs": [],
   "source": [
    "mamba_net = MambaLayerglobal(dim=256).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2cdf14ee-3c00-4476-9176-d1db743fe780",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn((4,256,30,32,32), device=device)\n",
    "y = mamba_net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "43e9aff4-098f-4e27-bd44-e3a6b0d3b0da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 5441.365234375 ms\n"
     ]
    }
   ],
   "source": [
    "start_event = torch.cuda.Event(enable_timing=True)\n",
    "end_event = torch.cuda.Event(enable_timing=True)\n",
    "start_event.record()\n",
    "\n",
    "# Run the operation\n",
    "for i in range(100):\n",
    "    y = mamba_net(x)\n",
    "    y_sum = torch.sum(y)\n",
    "    # y_sum.backward()\n",
    "# Record end event\n",
    "end_event.record()\n",
    "\n",
    "# Wait for everything to finish running\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "# Calculate elapsed time in milliseconds\n",
    "elapsed_time_ms = start_event.elapsed_time(end_event)\n",
    "print(f\"Elapsed time: {elapsed_time_ms} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79ad672-7ed9-48e0-8899-810e2a856a37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1097618f-8c97-479d-af8d-7ee3699c2247",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d488c9-964a-4d39-9f45-08614baeaee1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad685e16-2cdb-4179-94e5-a5fa104d6636",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
